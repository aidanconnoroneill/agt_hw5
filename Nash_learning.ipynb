{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-99,   1],\n",
       "        [ -1,   0]],\n",
       "\n",
       "       [[-99,  -1],\n",
       "        [  1,   0]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "games = np.load(\"games.npz\")\n",
    "prisoners_dilemma = games[\"prisoners_dilemma\"]\n",
    "penalty_kick = games[\"penalty_kick\"]\n",
    "chicken = games[\"chicken\"]\n",
    "bach_or_stravinsky = games[\"bach_or_stravinsky\"]\n",
    "rock_paper_scissors = games[\"rock_paper_scissors\"]\n",
    "five_by_five = games[\"five_by_five\"]\n",
    "two_by_three_by_four = games[\"two_by_three_by_four\"]\n",
    "four_players = games[\"four_players\"]\n",
    "chicken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.5, 0.5]), array([0.5, 0.5])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def uniform_profile(game):\n",
    "    return [np.ones(s)/s for s in game.shape[1:]]\n",
    "\n",
    "uniform_profile(two_by_three_by_four)\n",
    "uniform_profile(penalty_kick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviation_payoffs(game, profile):\n",
    "    dev_pays = []\n",
    "    for player in range(game.shape[0]):\n",
    "        payoffs = np.array(game[player])\n",
    "        probs = np.ones(game.shape[1:])\n",
    "        \n",
    "        #calculate outcome probabilities\n",
    "        for opponent in range(game.shape[0]):\n",
    "            if opponent == player:\n",
    "                continue\n",
    "            shape = [1] * game.shape[0]\n",
    "            shape[opponent] = game.shape[opponent+1]\n",
    "            probs *= profile[opponent].reshape(shape)\n",
    "            \n",
    "        payoffs = (payoffs * probs)\n",
    "        \n",
    "        #sum over opponent actions\n",
    "        for opponent in range(game.shape[0]):\n",
    "            if opponent < player:\n",
    "                payoffs = payoffs.sum(0)\n",
    "            elif opponent > player:\n",
    "                payoffs = payoffs.sum(1)\n",
    "        \n",
    "        dev_pays.append(payoffs)\n",
    "\n",
    "    return dev_pays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_utilities(game, profile):\n",
    "    dev_payoff = deviation_payoffs(game, profile)\n",
    "    for i, payoff in enumerate(dev_payoff):\n",
    "        dev_payoff[i] = (profile[i] * dev_payoff[i]).sum()\n",
    "    return dev_payoff\n",
    "def regrets(game, profile):\n",
    "    expected_utility = expected_utilities(game, profile)\n",
    "    deviations = deviation_payoffs(game, profile)\n",
    "    \n",
    "    regrets = []\n",
    "    for i, player in enumerate(deviations):\n",
    "        max_pure = max(deviations[i])\n",
    "        curr_regret = max(0, max_pure - expected_utility[i])\n",
    "        regrets.append(curr_regret)\n",
    "        \n",
    "    return regrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fictitious_play function runs fictitious play for max_iters iterations and returns the resulting profile of average strategies. If trace=True, it should also return the sequence of max_iters+1 profiles generated along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "TODO",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8da3788aebfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TODO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfictitious_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty_kick\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-8da3788aebfb>\u001b[0m in \u001b[0;36mfictitious_play\u001b[0;34m(game, prior_counts, max_iters, trace)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprior_counts\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprior_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TODO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfictitious_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty_kick\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: TODO"
     ]
    }
   ],
   "source": [
    "# TODO: https://www.youtube.com/watch?v=97Am0KZeSq8&feature=youtu.be 3:07\n",
    "def fictitious_play(game, prior_counts=None, max_iters=1000, trace=False):\n",
    "    if prior_counts == None:\n",
    "        prior_counts = [np.ones(s, dtype=int) for s in game.shape[1:]]\n",
    "    raise NotImplementedError(\"TODO\")\n",
    "\n",
    "fictitious_play(penalty_kick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The replicator_dynamics function runs replicator dynamics for max_iters iterations and returns the resulting profile. If trace=True, it should also return the sequence of max_iters+1 profiles generated along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a Nash Equilibrium profile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.01010144, 0.98989856]), array([0.01010144, 0.98989856])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replicator_dynamics(game, initial_profile=None, max_iters=250, trace=False):\n",
    "    if initial_profile == None:\n",
    "        initial_profile = uniform_profile(game)\n",
    "\n",
    "    profiles = [initial_profile]\n",
    "    num_players = game.shape[0]\n",
    "    offset = [np.min(game[i]) for i in range(num_players)]\n",
    "    for T in range(max_iters):\n",
    "        current_profile = profiles[-1]\n",
    "        deviations = deviation_payoffs(game, current_profile)\n",
    "        weights = [np.array([current_profile[player][action] * \n",
    "                   (deviations[player][action] - offset[player]) \n",
    "                   for action in range(game.shape[player+1])]) for player in range(num_players)]\n",
    "        normalized_weights = [np.array([weight/row.sum() for weight in row]) for row in weights]\n",
    "        profiles.append(normalized_weights)\n",
    "        \n",
    "        calc_regrets = regrets(game, normalized_weights)\n",
    "        if sum(calc_regrets) == 0:\n",
    "            break\n",
    "    calc_regrets = regrets(game, profiles[-1])\n",
    "    if sum(calc_regrets) == 0:\n",
    "            print('converged to Nash Equilibrium')\n",
    "    else:\n",
    "        print('Not a Nash Equilibrium profile')\n",
    "    if trace == False:\n",
    "        return profiles[-1]\n",
    "    else:\n",
    "        return profiles[-1], profiles \n",
    "            \n",
    "\n",
    "replicator_dynamics(chicken, max_iters=1000, trace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RD_random_restarts function tries to find as many Nash equilibria as possible by running replicator dynamics many times. Each run begins from a different randomly-generated profile. The random_profile function should generate these profiles by sampling weights from a uniform distribution and normalizing them into probability distributions for each player's strategy.\n",
    "\n",
    "Only distinct Nash equilibria will be returned. This means that the profiles returned by replicator_dynamics must be checked to 1) confirm that they are Nash equilibria (you should use your regret function from hw1) and 2) that they are distinct (you should use np.allclose() with atol=min_dist on each player's strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_profile(game):\n",
    "    raise NotImplementedError(\"TODO\")\n",
    "\n",
    "    #check for NE, keep ones that are unique, throw out duplicates and non NE\n",
    "def RD_random_restarts(game, num_restarts=100, max_regret=0.01, min_dist=0.01, **rd_kwds):\n",
    "    raise NotImplementedError(\"TODO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSimplex(points, fig=None, \n",
    "                vertexlabels=['1','2','3'],\n",
    "                **kwargs):\n",
    "    \"\"\"\n",
    "    Plot Nx3 points array on the 3-simplex \n",
    "    (with optionally labeled vertices) \n",
    "    \n",
    "    kwargs will be passed along directly to matplotlib.pyplot.plot    \n",
    "    Returns Figure, caller must .show(), unless in a notebook\n",
    "    \"\"\"\n",
    "    if(fig == None):        \n",
    "        fig = plt.figure()\n",
    "    # Draw the triangle\n",
    "    l1 = mpl.lines.Line2D([0, 0.5, 1.0, 0], # xcoords\n",
    "                  [0, np.sqrt(3) / 2, 0, 0], # ycoords\n",
    "                  color='k')\n",
    "    fig.gca().add_line(l1)\n",
    "    fig.gca().xaxis.set_major_locator(mpl.ticker.NullLocator())\n",
    "    fig.gca().yaxis.set_major_locator(mpl.ticker.NullLocator())\n",
    "    # Draw vertex labels\n",
    "    fig.gca().text(-0.05, -0.05, vertexlabels[0])\n",
    "    fig.gca().text(1.05, -0.05, vertexlabels[1])\n",
    "    fig.gca().text(0.5, np.sqrt(3) / 2 + 0.05, vertexlabels[2])\n",
    "    # Project and draw the actual points\n",
    "    projected = projectSimplex(points)\n",
    "    plt.plot(projected[:,0], projected[:,1], **kwargs)              \n",
    "    # Leave some buffer around the triangle for vertex labels\n",
    "    fig.gca().set_xlim(-0.2, 1.2)\n",
    "    fig.gca().set_ylim(-0.2, 1.2)\n",
    "\n",
    "    return fig    \n",
    "\n",
    "\n",
    "def projectSimplex(points):\n",
    "    \"\"\" \n",
    "    Project probabilities on the 3-simplex to a 2D triangle\n",
    "    \n",
    "    N points are given as N x 3 array\n",
    "    \"\"\"\n",
    "    # Convert points one at a time\n",
    "    tripts = np.zeros((points.shape[0],2))\n",
    "    for idx in range(points.shape[0]):\n",
    "        # Init to triangle centroid\n",
    "        x = 1.0 / 2\n",
    "        y = 1.0 / (2 * np.sqrt(3))\n",
    "        # Vector 1 - bisect out of lower left vertex \n",
    "        p1 = points[idx, 0]\n",
    "        x = x - (1.0 / np.sqrt(3)) * p1 * np.cos(np.pi / 6)\n",
    "        y = y - (1.0 / np.sqrt(3)) * p1 * np.sin(np.pi / 6)\n",
    "        # Vector 2 - bisect out of lower right vertex  \n",
    "        p2 = points[idx, 1]  \n",
    "        x = x + (1.0 / np.sqrt(3)) * p2 * np.cos(np.pi / 6)\n",
    "        y = y - (1.0 / np.sqrt(3)) * p2 * np.sin(np.pi / 6)        \n",
    "        # Vector 3 - bisect out of top vertex\n",
    "        p3 = points[idx, 2]\n",
    "        y = y + (1.0 / np.sqrt(3) * p3)\n",
    "      \n",
    "        tripts[idx,:] = (x,y)\n",
    "\n",
    "    return tripts\n",
    "\n",
    "\n",
    "def random_walk(symmetric_profile, length):\n",
    "    walk = [symmetric_profile]\n",
    "    for _ in range(length):\n",
    "        prof = walk[-1]\n",
    "        new_prof = prof + np.random.uniform(-.1,.1,prof.shape)\n",
    "        if(new_prof.min() < 0):\n",
    "            new_prof -= new_prof.min()\n",
    "        new_prof /= new_prof.sum()\n",
    "        walk.append(new_prof)\n",
    "    return np.array(walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
